{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c6aaaf",
   "metadata": {
    "papermill": {
     "duration": 0.005357,
     "end_time": "2024-12-11T20:39:26.422204",
     "exception": false,
     "start_time": "2024-12-11T20:39:26.416847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Group 9 - Omkar Chaudhari, Akshay Gurumoorthi, Rishabh Puri, Matthew Too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8895898",
   "metadata": {
    "papermill": {
     "duration": 0.005696,
     "end_time": "2024-12-11T20:39:26.432508",
     "exception": false,
     "start_time": "2024-12-11T20:39:26.426812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Property Driven Molecule Generation using Conditional Normalizing Flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824eae1c",
   "metadata": {
    "papermill": {
     "duration": 0.004254,
     "end_time": "2024-12-11T20:39:26.441109",
     "exception": false,
     "start_time": "2024-12-11T20:39:26.436855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Abstract\n",
    "This work presents a method to generate molecules with target properties using Conditional\n",
    "Normalising Flows. The properties of the generated molecules are further validated using xtb,\n",
    "and ORCA. Using the QM9 dataset, an autorgressive normalising flow model is trained on\n",
    "the molecules using TensorFlow and DeepChem, with the one hot encodings of their SELF-\n",
    "IES strings. The model is conditioned during training with DFT computed properties of the\n",
    "molecules from the dataset. By utilising the learned bijections, new molecules are sampled by\n",
    "passing a condition vector with properties of interest. Using the models outputted SELFIES\n",
    "strings, these are converted back to SMILES, created into ORCA input files using RDKit and\n",
    "Python, and validated using ORCA. This conditioning approach is tested to see if the model can\n",
    "be directed towards generating molecules with desired properties. Such models will be useful\n",
    "to generate new molecules in various domains like drug discovery, the semiconductor industry,\n",
    "renewable materials etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8213c1e",
   "metadata": {
    "papermill": {
     "duration": 0.004262,
     "end_time": "2024-12-11T20:39:26.449709",
     "exception": false,
     "start_time": "2024-12-11T20:39:26.445447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Code Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5020aa5",
   "metadata": {
    "papermill": {
     "duration": 0.004187,
     "end_time": "2024-12-11T20:39:26.458222",
     "exception": false,
     "start_time": "2024-12-11T20:39:26.454035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The base code was used from Deepchems normalising flow tutorial - https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Training_a_Normalizing_Flow_on_QM9.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfdaaf6",
   "metadata": {
    "papermill": {
     "duration": 0.004312,
     "end_time": "2024-12-11T20:39:26.466898",
     "exception": false,
     "start_time": "2024-12-11T20:39:26.462586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The initial smiles processing is the same as the tutorial, but the model implementation has been changed and written by us. The conditioning part had not been implemented into the deepchem library's normalising flow function.\n",
    "\n",
    "Additionally we have added more parameter controls, early stopping, and have added the DFT conditioning sampling from the model. The nfm.flow.sample() by deepchem cannot sample conditioned samples. So we used tensorflows tfd.TransformedDistribution.sample to sample the conditioned molecule from the learned chained bijectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345f9a57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:39:26.477852Z",
     "iopub.status.busy": "2024-12-11T20:39:26.477033Z",
     "iopub.status.idle": "2024-12-11T20:39:27.190119Z",
     "shell.execute_reply": "2024-12-11T20:39:27.189345Z"
    },
    "papermill": {
     "duration": 0.720793,
     "end_time": "2024-12-11T20:39:27.192077",
     "exception": false,
     "start_time": "2024-12-11T20:39:26.471284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a10236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:39:27.202632Z",
     "iopub.status.busy": "2024-12-11T20:39:27.202231Z",
     "iopub.status.idle": "2024-12-11T20:39:27.205992Z",
     "shell.execute_reply": "2024-12-11T20:39:27.205321Z"
    },
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1733352448907,
     "user": {
      "displayName": "Matthew Too",
      "userId": "17987340176483548953"
     },
     "user_tz": 360
    },
    "id": "Jq3h8l8YKFul",
    "papermill": {
     "duration": 0.010792,
     "end_time": "2024-12-11T20:39:27.207565",
     "exception": false,
     "start_time": "2024-12-11T20:39:27.196773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49950e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T04:24:54.820020Z",
     "iopub.status.busy": "2024-12-06T04:24:54.819670Z",
     "iopub.status.idle": "2024-12-06T04:25:21.332867Z",
     "shell.execute_reply": "2024-12-06T04:25:21.331428Z",
     "shell.execute_reply.started": "2024-12-06T04:24:54.819989Z"
    },
    "executionInfo": {
     "elapsed": 29769,
     "status": "ok",
     "timestamp": 1733352479839,
     "user": {
      "displayName": "Matthew Too",
      "userId": "17987340176483548953"
     },
     "user_tz": 360
    },
    "id": "i3Y1jLxbKFum",
    "outputId": "70e07731-5673-4f88-ebf3-b5b40b07fb44",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-12-11T20:39:27.212059",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install --pre deepchem\n",
    "!pip install selfies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281356f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import deepchem as dc\n",
    "from deepchem.data import NumpyDataset\n",
    "from deepchem.splits import RandomSplitter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import selfies as sf\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from rdkit.Chem.Fingerprints.FingerprintMols import FingerprintMol\n",
    "from rdkit.DataStructs import FingerprintSimilarity\n",
    "from IPython.display import display\n",
    "\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras\n",
    "\n",
    "tfk.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f3897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T04:25:21.334913Z",
     "iopub.status.busy": "2024-12-06T04:25:21.334537Z",
     "iopub.status.idle": "2024-12-06T04:30:44.599804Z",
     "shell.execute_reply": "2024-12-06T04:30:44.598360Z",
     "shell.execute_reply.started": "2024-12-06T04:25:21.334875Z"
    },
    "executionInfo": {
     "elapsed": 247279,
     "status": "ok",
     "timestamp": 1733352749708,
     "user": {
      "displayName": "Matthew Too",
      "userId": "17987340176483548953"
     },
     "user_tz": 360
    },
    "id": "vTRD3366KFun",
    "outputId": "f52ad45b-210b-4f67-96af-c994c011060a",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_qm9(featurizer='ECFP')\n",
    "qm9_dataset = datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eeb9c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T04:30:44.602534Z",
     "iopub.status.busy": "2024-12-06T04:30:44.602033Z",
     "iopub.status.idle": "2024-12-06T04:30:44.657853Z",
     "shell.execute_reply": "2024-12-06T04:30:44.656686Z",
     "shell.execute_reply.started": "2024-12-06T04:30:44.602495Z"
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1733352864136,
     "user": {
      "displayName": "Matthew Too",
      "userId": "17987340176483548953"
     },
     "user_tz": 360
    },
    "id": "PZteNX6QxItr",
    "outputId": "e654af29-6e91-4688-c68e-18ec96f57f5c",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(qm9_dataset.y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e9844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T04:33:45.684349Z",
     "iopub.status.busy": "2024-12-06T04:33:45.683899Z",
     "iopub.status.idle": "2024-12-06T04:33:45.742811Z",
     "shell.execute_reply": "2024-12-06T04:33:45.741507Z",
     "shell.execute_reply.started": "2024-12-06T04:33:45.684312Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qm9_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495dad86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:09:16.623987Z",
     "iopub.status.busy": "2024-12-06T05:09:16.621639Z",
     "iopub.status.idle": "2024-12-06T05:09:16.634453Z",
     "shell.execute_reply": "2024-12-06T05:09:16.633054Z",
     "shell.execute_reply.started": "2024-12-06T05:09:16.623929Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_names = qm9_dataset.get_task_names()\n",
    "print(task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3aef1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:09:20.654164Z",
     "iopub.status.busy": "2024-12-06T05:09:20.653767Z",
     "iopub.status.idle": "2024-12-06T05:09:20.689985Z",
     "shell.execute_reply": "2024-12-06T05:09:20.688563Z",
     "shell.execute_reply.started": "2024-12-06T05:09:20.654132Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "zero_mask = (qm9_dataset.w == 0)\n",
    "zero_count = np.sum(zero_mask)\n",
    "print(zero_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7c3ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:09:23.268964Z",
     "iopub.status.busy": "2024-12-06T05:09:23.268473Z",
     "iopub.status.idle": "2024-12-06T05:09:23.441552Z",
     "shell.execute_reply": "2024-12-06T05:09:23.440194Z",
     "shell.execute_reply.started": "2024-12-06T05:09:23.268930Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size = 20000\n",
    "df = pd.DataFrame(data={'smiles': qm9_dataset.ids})\n",
    "dft_features = pd.DataFrame(data=qm9_dataset.y, columns=task_names)\n",
    "data = pd.concat([df[['smiles']], dft_features], axis=1).sample(dataset_size, random_state=42)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a2aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:09:25.685102Z",
     "iopub.status.busy": "2024-12-06T05:09:25.684549Z",
     "iopub.status.idle": "2024-12-06T05:09:25.735051Z",
     "shell.execute_reply": "2024-12-06T05:09:25.733740Z",
     "shell.execute_reply.started": "2024-12-06T05:09:25.685051Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0ef68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:33:25.206673Z",
     "iopub.status.busy": "2024-12-06T05:33:25.206113Z",
     "iopub.status.idle": "2024-12-06T05:33:25.229798Z",
     "shell.execute_reply": "2024-12-06T05:33:25.228613Z",
     "shell.execute_reply.started": "2024-12-06T05:33:25.206634Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampled_data = data.sample(1, random_state=42).reset_index(drop=True)\n",
    "sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeca7a4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_smiles(smiles):\n",
    "    if '.' in smiles:\n",
    "        return None\n",
    "    return sf.encoder(smiles)\n",
    "\n",
    "def keys_int(symbol_to_int):\n",
    "    return {i: key for i, key in enumerate(symbol_to_int.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45aaf49",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data['selfies'] = data['smiles'].apply(preprocess_smiles)\n",
    "data.dropna(subset=['selfies'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data['len'] = data['smiles'].str.len()\n",
    "data.sort_values(by='len', inplace=True)\n",
    "\n",
    "\n",
    "constraints = sf.get_semantic_constraints()\n",
    "constraints['?'] = 3\n",
    "sf.set_semantic_constraints(constraints)\n",
    "\n",
    "print(sf.set_semantic_constraints())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8834b93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selfies_list = data['selfies'].to_numpy()\n",
    "selfies_alphabet = sf.get_alphabet_from_selfies(selfies_list)\n",
    "selfies_alphabet.add('[nop]')\n",
    "selfies_alphabet = sorted(selfies_alphabet)\n",
    "\n",
    "largest_selfie_len = max(sf.len_selfies(s) for s in selfies_list)\n",
    "symbol_to_int = {c: i for i, c in enumerate(selfies_alphabet)}\n",
    "onehots = sf.batch_selfies_to_flat_hot(selfies_list, symbol_to_int, largest_selfie_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e42e25",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dft_features_values = data[task_names].to_numpy(dtype='float64')\n",
    "combined_data = np.hstack([onehots, dft_features_values])\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(onehots, dtype='float64')\n",
    "noise_tensor = tf.random.uniform(shape=input_tensor.shape, minval=0, maxval=1, dtype='float64')\n",
    "dequantized_data = tf.add(input_tensor, noise_tensor)\n",
    "\n",
    "# Convert to tensors and prepare data for training\n",
    "input_tensor = tf.convert_to_tensor(dequantized_data, dtype='float64')\n",
    "dft_tensor = tf.convert_to_tensor(dft_features_values, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ccf67",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = NumpyDataset(X=input_tensor, y=dft_tensor)\n",
    "splitter = RandomSplitter()\n",
    "train, val, test = splitter.train_valid_test_split(dataset=dataset, seed=42)\n",
    "print(\"Training, validation, and test splits created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674eb33",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim_smiles = input_tensor.shape[-1]\n",
    "dim_dft = dft_tensor.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ddbc4b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_shape = (dim_smiles,)\n",
    "conditional_event_shape = (dim_dft,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc00e4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T02:12:34.738987Z",
     "iopub.status.busy": "2024-11-29T02:12:34.738522Z",
     "iopub.status.idle": "2024-11-29T02:13:44.133785Z",
     "shell.execute_reply": "2024-11-29T02:13:44.132672Z",
     "shell.execute_reply.started": "2024-11-29T02:12:34.738947Z"
    },
    "executionInfo": {
     "elapsed": 268366,
     "status": "ok",
     "timestamp": 1733353868375,
     "user": {
      "displayName": "Matthew Too",
      "userId": "17987340176483548953"
     },
     "user_tz": 360
    },
    "id": "J94V5zrHKFup",
    "outputId": "265bbbae-f6ca-44d5-b8f2-b79cd68ae95c",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_layers = 12\n",
    "hidden_units=[128,128]\n",
    "flow_layers = []\n",
    "\n",
    "Made = tfb.AutoregressiveNetwork(params=2, hidden_units=hidden_units, activation='relu', event_shape=event_shape, conditional=True, conditional_event_shape=conditional_event_shape, conditional_input_layers='all_layers')\n",
    "\n",
    "for i in range(num_layers):\n",
    "    flow_layers.append(tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn=Made, name=f'maf{i}'))\n",
    "    flow_layers.append(tfb.Permute(tf.cast(np.random.permutation(np.arange(0, dim_smiles)), tf.int32)))\n",
    "\n",
    "print(\"Flow layers defined.\")\n",
    "\n",
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(dim_smiles, dtype=tf.float64), scale_diag=tf.ones(dim_smiles, dtype=tf.float64))\n",
    "chain_bijector = tfb.Chain(list(reversed(flow_layers)))\n",
    "distribution = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=chain_bijector\n",
    ")\n",
    "\n",
    "import re\n",
    "def make_bijector_kwargs(bijector, name_to_kwargs):\n",
    "  if hasattr(bijector, 'bijectors'):\n",
    "    return {b.name: make_bijector_kwargs(b, name_to_kwargs) for b in bijector.bijectors}\n",
    "  else:\n",
    "    for name_regex, kwargs in name_to_kwargs.items():\n",
    "      if re.match(name_regex, bijector.name):\n",
    "        return kwargs\n",
    "  return {}\n",
    "\n",
    "# Construct and compile the conditional normalizing flow model\n",
    "x_ = tfk.Input(shape=(dim_smiles,), dtype=tf.float64)\n",
    "c_ = tfk.Input(shape=(dim_dft,), dtype=tf.float64)\n",
    "log_prob_ = distribution.log_prob(x_, bijector_kwargs=make_bijector_kwargs(chain_bijector, {'maf.': {'conditional_input': c_}}))\n",
    "model = tfk.Model([x_, c_], log_prob_)\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=lambda _, log_prob: -log_prob)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',       # Monitor validation loss\n",
    "    patience=5,               # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True # Restore the best weights after stopping\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "max_epochs = 100\n",
    "n = len(train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e08ff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    x=[\n",
    "        train.X, \n",
    "        (train.y - np.mean(train.y, axis=0)) / np.std(train.y, axis=0)\n",
    "    ],\n",
    "    y=np.zeros((n, 0), dtype=np.float64),\n",
    "    batch_size=batch_size,\n",
    "    epochs=max_epochs,\n",
    "    steps_per_epoch=n // batch_size,\n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    validation_data=[\n",
    "        [val.X, val.y], \n",
    "        np.zeros((len(val.X), 0))\n",
    "    ],\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40db3a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plot_filename = f'training_validation_loss_layers{num_layers}_units{hidden_units}.png'\n",
    "plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d20dc0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "\n",
    "def sample_and_save(dataset, num_samples=5):\n",
    "    # Convert dataset to a NumPy array with float64 for numerical operations\n",
    "    dataset = np.array(dataset, dtype=object)  # Convert to a general NumPy array first\n",
    "    smiles = dataset[:, 0]  # Extract the SMILES strings\n",
    "    features = dataset[:, 1:].astype('float64')  # Convert the rest to float64 for numerical processing\n",
    "    \n",
    "    # Column names for DFT conditioning values\n",
    "    dft_columns = ['mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'cv', 'u0', 'u298', 'h298', 'g298']\n",
    "    conditioning_mean = np.mean(features, axis=0)\n",
    "    conditioning_std = np.std(features, axis=0)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for vector_num, (smile, conditioning_values) in enumerate(zip(smiles, features)):\n",
    "        conditioning_values_scaled = (conditioning_values - conditioning_mean) / conditioning_std\n",
    "        \n",
    "        condition_vector = tf.convert_to_tensor(\n",
    "            np.tile(conditioning_values_scaled, (num_samples, 1)),\n",
    "            dtype=tf.float64\n",
    "        )\n",
    "\n",
    "        sampled_smiles = distribution.sample(\n",
    "            (num_samples,),\n",
    "            bijector_kwargs=make_bijector_kwargs(chain_bijector, {'maf.': {'conditional_input': condition_vector}})\n",
    "        )\n",
    "        sampled_smiles = tf.math.floor(sampled_smiles)\n",
    "        sampled_smiles = tf.clip_by_value(sampled_smiles, 0, 1)\n",
    "        mols_list = sampled_smiles.numpy().tolist()\n",
    "        \n",
    "        for mol in mols_list:\n",
    "            for j in range(largest_selfie_len):\n",
    "                row = mol[len(selfies_alphabet) * j: len(selfies_alphabet) * (j + 1)]\n",
    "                if all(elem == 0 for elem in row):\n",
    "                    mol[len(selfies_alphabet) * (j+1) - 1] = 1\n",
    "\n",
    "        int_mol = keys_int(symbol_to_int)\n",
    "        generated_selfies = sf.batch_flat_hot_to_selfies(mols_list, int_mol)\n",
    "        valid_selfies, valid_smiles = [], []\n",
    "\n",
    "        for selfies in generated_selfies:\n",
    "            try:\n",
    "                smiles = sf.decoder(selfies)\n",
    "                if Chem.MolFromSmiles(smiles, sanitize=True) is not None:\n",
    "                    valid_selfies.append(selfies)\n",
    "                    valid_smiles.append(smiles)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        gen_mols = [Chem.MolFromSmiles(sm) for sm in valid_smiles]\n",
    "        original_mol = Chem.MolFromSmiles(smile)\n",
    "\n",
    "        # Compute Tanimoto similarity between generated molecules and the original molecule\n",
    "        def tanimoto_similarity(query_mol, generated_mols):\n",
    "            query_fp = FingerprintMol(query_mol)\n",
    "            similarities = []\n",
    "            for gen_mol in generated_mols:\n",
    "                if gen_mol:\n",
    "                    gen_fp = FingerprintMol(gen_mol)\n",
    "                    similarities.append(FingerprintSimilarity(query_fp, gen_fp))\n",
    "                else:\n",
    "                    similarities.append(None)  # Handle invalid molecules\n",
    "            return similarities\n",
    "\n",
    "        similarities = tanimoto_similarity(original_mol, gen_mols)\n",
    "\n",
    "        # Save the image for the original SMILES\n",
    "        if original_mol:\n",
    "            original_filename = f\"original_layers{num_layers}_units{hidden_units}_vector{vector_num}.png\"\n",
    "            Draw.MolToFile(original_mol, original_filename)\n",
    "\n",
    "        # Save images for the generated molecules\n",
    "        for sample_num, mol in enumerate(gen_mols):\n",
    "            if mol:\n",
    "                generated_filename = f\"generated_layers{num_layers}_units{hidden_units}_sample{sample_num + 1}_vector{vector_num}.png\"\n",
    "                Draw.MolToFile(mol, generated_filename)\n",
    "\n",
    "        # Append results for each molecule\n",
    "        results.append({\n",
    "            \"Original_SMILES\": smile,\n",
    "            **{col: val for col, val in zip(dft_columns, conditioning_values)},\n",
    "            **{f\"Generated_SMILES_{i+1}\": valid_smiles[i] if i < len(valid_smiles) else None for i in range(num_samples)},\n",
    "            **{f\"Similarity_Score_{i+1}\": round(similarities[i], 3) if i < len(similarities) and similarities[i] is not None else None for i in range(num_samples)},\n",
    "        })\n",
    "\n",
    "    # Convert results into a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    output_file = f\"generated_molecules_layers{num_layers}_units{hidden_units}.xlsx\"\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Results and images saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151435cb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_and_save(sampled_data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-11T20:39:23.982695",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}